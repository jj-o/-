{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "184a9Vd8zks6v0uCDu13T2V8hws4Vpok8",
      "authorship_tag": "ABX9TyMpwOekiivGXi/NB5aw8AE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jj-o/Pytorch/blob/main/%EC%83%9D%EC%84%B1%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l32dk6dD0mF8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Pytorch/chap13/data.zip -d data/"
      ],
      "metadata": {
        "id": "goTnHcfH2QTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/Pytorch/chap13/data\", train=True, transform=transform, download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/Pytorch/chap13/data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "9Y-KUhYJ2g5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            nn.Linear(3 * 3 * 32, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, encoded_space_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "    \n",
        "class Decoder(nn.Module):    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 3 * 3 * 32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, \n",
        "        unflattened_size=(32, 3, 3))\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 3, \n",
        "            stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
        "            padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
        "            padding=1, output_padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        x = self.unflatten(x)\n",
        "        x = self.decoder_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9itISAnl29Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(encoded_space_dim=4,fc2_input_dim=128)\n",
        "decoder = Decoder(encoded_space_dim=4,fc2_input_dim=128)\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "params_to_optimize = [\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "]\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=0.001, weight_decay=1e-05)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "QStp2i7F3Aif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    for image_batch, _ in dataloader: \n",
        "        image_noisy = add_noise(image_batch,noise_factor)\n",
        "        image_noisy = image_noisy.to(device)    \n",
        "        encoded_data = encoder(image_noisy)\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        loss = loss_fn(decoded_data, image_noisy)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "    return np.mean(train_loss)"
      ],
      "metadata": {
        "id": "1ROLYtHi3D3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(encoder, decoder, device, dataloader, loss_fn,noise_factor=0.3):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad(): \n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch, _ in dataloader:\n",
        "            image_batch = image_batch.to(device)\n",
        "            encoded_data = encoder(image_batch)\n",
        "            decoded_data = decoder(encoded_data)\n",
        "            conc_out.append(decoded_data.cpu())\n",
        "            conc_label.append(image_batch.cpu())\n",
        "        conc_out = torch.cat(conc_out)\n",
        "        conc_label = torch.cat(conc_label) \n",
        "        val_loss = loss_fn(conc_out, conc_label)\n",
        "    return val_loss.data"
      ],
      "metadata": {
        "id": "SxtvANqz3Jle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(inputs,noise_factor=0.3):\n",
        "    noisy = inputs+torch.randn_like(inputs) * noise_factor\n",
        "    noisy = torch.clip(noisy,0.,1.)\n",
        "    return noisy"
      ],
      "metadata": {
        "id": "Ya1SmeIB3SLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ae_outputs(encoder,decoder,n=5,noise_factor=0.3):\n",
        "    plt.figure(figsize=(10,4.5))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(3,n,i+1)\n",
        "        img = test_dataset[i][0].unsqueeze(0)\n",
        "        image_noisy = add_noise(img,noise_factor)     \n",
        "        image_noisy = image_noisy.to(device)\n",
        "\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            rec_img  = decoder(encoder(image_noisy))\n",
        "\n",
        "        plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)  \n",
        "        if i == n//2:\n",
        "            ax.set_title('원래 이미지')\n",
        "        ax = plt.subplot(3, n, i + 1 + n)\n",
        "        plt.imshow(image_noisy.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)  \n",
        "        if i == n//2:\n",
        "            ax.set_title('노이즈가 적용되어 손상된 이미지')\n",
        "\n",
        "        ax = plt.subplot(3, n, i + 1 + n + n)\n",
        "        plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)  \n",
        "        if i == n//2:\n",
        "            ax.set_title('재구성된 이미지')\n",
        "    plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1, \n",
        "                    right=0.7, \n",
        "                    top=0.9, \n",
        "                    wspace=0.3, \n",
        "                    hspace=0.3)     \n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "1ngNJVgr3iAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_epochs = 30\n",
        "history_da={'train_loss':[],'val_loss':[]}\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss=train_epoch(\n",
        "        encoder=encoder, \n",
        "        decoder=decoder, \n",
        "        device=device, \n",
        "        dataloader=train_loader, \n",
        "        loss_fn=loss_fn, \n",
        "        optimizer=optim, noise_factor=0.3)\n",
        "    val_loss = test_epoch(\n",
        "        encoder=encoder, \n",
        "        decoder=decoder, \n",
        "        device=device, \n",
        "        dataloader=test_loader, \n",
        "        loss_fn=loss_fn, noise_factor=0.3)\n",
        "    history_da['train_loss'].append(train_loss)\n",
        "    history_da['val_loss'].append(val_loss)\n",
        "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "    plot_ae_outputs(encoder,decoder,noise_factor=0.3)"
      ],
      "metadata": {
        "id": "KM_E5Iuw3nwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GAN"
      ],
      "metadata": {
        "id": "R6K1gU0u4Is-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ZqwVmi8s4MaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "epochs = 200\n",
        "sample_size = 64 \n",
        "nz = 128 \n",
        "k = 1 "
      ],
      "metadata": {
        "id": "pLedLSDe4czz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"/content/drive/MyDrive/Pytorch/chap13/data\", train=True, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "jWnMD4H24kje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.nz, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "oqX0G-LH4qKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_input = 784\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.n_input, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        return self.main(x)"
      ],
      "metadata": {
        "id": "kKUlJm694xZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "print(generator)\n",
        "print(discriminator)"
      ],
      "metadata": {
        "id": "MJKEro_Y4zd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] \n",
        "losses_d = [] \n",
        "images = [] "
      ],
      "metadata": {
        "id": "Nwif018E5Bmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_generator_image(image, path):\n",
        "    save_image(image, path)"
      ],
      "metadata": {
        "id": "S7nt8cjs5E5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = torch.ones(b_size, 1).to(device)\n",
        "    fake_label = torch.zeros(b_size, 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output_real = discriminator(data_real)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "    return loss_real + loss_fake"
      ],
      "metadata": {
        "id": "4aJx-AZo5ImV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)    \n",
        "    real_label = torch.ones(b_size, 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "W9kBvRdi5hww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir img #img 폴더 만들기"
      ],
      "metadata": {
        "id": "EyEtlwWg5nyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for idx, data in tqdm(enumerate(train_loader), total=int(len(train_dataset)/train_loader.batch_size)):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        for step in range(k):                                \n",
        "            data_fake = generator(torch.randn(b_size, nz).to(device)).detach()\n",
        "            data_real = image\n",
        "            loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(torch.randn(b_size, nz).to(device))\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "    generated_img = generator(torch.randn(b_size, nz).to(device)).cpu().detach()\n",
        "    generated_img = make_grid(generated_img)\n",
        "    save_generator_image(generated_img, \"./img/gen_img{epoch}.png\")\n",
        "    images.append(generated_img)\n",
        "    epoch_loss_g = loss_g / idx \n",
        "    epoch_loss_d = loss_d / idx \n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "    \n",
        "    print(f\"Epoch {epoch} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")"
      ],
      "metadata": {
        "id": "7X4tLIyU5pef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
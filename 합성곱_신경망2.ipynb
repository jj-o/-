{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1ALAkH06RQdiAyBxRpIw3BRZFhiBMY_dr",
      "authorship_tag": "ABX9TyPZLAkCGua3IywZd2OGOR0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jj-o/Pytorch/blob/main/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LeNet-5\n"
      ],
      "metadata": {
        "id": "p1qSlRTC5Tno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zrR5J6wminS"
      },
      "outputs": [],
      "source": [
        "pip install --user tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "T9WaSZyBnZzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform():    \n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.data_transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(resize),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        }\n",
        "        \n",
        "    def __call__(self, img, phase):\n",
        "        return self.data_transform[phase](img)"
      ],
      "metadata": {
        "id": "7p8mriw7oydg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Pytorch/chap06/data"
      ],
      "metadata": {
        "id": "JeeqM6QyrReo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "CpRv_ciIrnaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/drive/MyDrive/Pytorch/chap06/dogs-vs-cats.zip'"
      ],
      "metadata": {
        "id": "Gj0bWtiQru5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_directory = '/content/drive/MyDrive/Pytorch/chap06/data/Cat'\n",
        "dog_directory = '/content/drive/MyDrive/Pytorch/chap06/data/Dog'\n",
        "\n",
        "cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])   \n",
        "dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
        "images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]    \n",
        "correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]    \n",
        "\n",
        "random.seed(42)    \n",
        "random.shuffle(correct_images_filepaths)\n",
        "#train_images_filepaths = correct_images_filepaths[:20000] #성능을 향상시키고 싶다면 훈련 데이터셋을 늘려서 테스트해보세요   \n",
        "#val_images_filepaths = correct_images_filepaths[20000:-10] #훈련과 함께 검증도 늘려줘야 합니다\n",
        "train_images_filepaths = correct_images_filepaths[:400]    \n",
        "val_images_filepaths = correct_images_filepaths[400:-10]  \n",
        "test_images_filepaths = correct_images_filepaths[-10:]    \n",
        "print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))"
      ],
      "metadata": {
        "id": "dR1mlgGRpmvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
        "    rows = len(images_filepaths) // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths):\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]\n",
        "        predicted_label = predicted_labels[i] if predicted_labels else true_label\n",
        "        color = \"green\" if true_label == predicted_label else \"red\"\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(predicted_label, color=color)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0EwEFk8gtKRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepaths)"
      ],
      "metadata": {
        "id": "iUAGfKJTtadA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogvsCatDataset(Dataset):    \n",
        "    def __init__(self, file_list, transform=None, phase='train'):    \n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):       \n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)        \n",
        "        img_transformed = self.transform(img, self.phase)\n",
        "        \n",
        "        label = img_path.split('/')[-1].split('.')[0]\n",
        "        if label == 'dog':\n",
        "            label = 1\n",
        "        elif label == 'cat':\n",
        "            label = 0\n",
        "        return img_transformed, label"
      ],
      "metadata": {
        "id": "ZgjPZh9HtlaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "vSfAOVegtnKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')\n",
        "val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')\n",
        "\n",
        "index = 0\n",
        "print(train_dataset.__getitem__(index)[0].size())\n",
        "print(train_dataset.__getitem__(index)[1])"
      ],
      "metadata": {
        "id": "IgKEl18PuKiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
        "\n",
        "batch_iterator = iter(train_dataloader)\n",
        "inputs, label = next(batch_iterator)\n",
        "print(inputs.size())\n",
        "print(label)"
      ],
      "metadata": {
        "id": "9Lu0N2JAuL5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0) \n",
        "        self.relu1 = nn.ReLU() \n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2) \n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0) \n",
        "        self.relu2 = nn.ReLU() # activation\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)         \n",
        "        self.fc1 = nn.Linear(32*53*53, 512) \n",
        "        self.relu5 = nn.ReLU()         \n",
        "        self.fc2 = nn.Linear(512, 2) \n",
        "        self.output = nn.Softmax(dim=1)        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.cnn1(x) \n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.cnn2(out) \n",
        "        out = self.relu2(out) \n",
        "        out = self.maxpool2(out) \n",
        "        out = out.view(out.size(0), -1) \n",
        "        out = self.fc1(out) \n",
        "        out = self.fc2(out)                    \n",
        "        out = self.output(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "v_EyM6a9u6zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "-1FgE1nMvmd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchsummary"
      ],
      "metadata": {
        "id": "939INB4LvtcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "1Cv77mI8vwRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "rFS1rVzQv5Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "dhY6PCCwwOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):    \n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epoch))\n",
        "        print('-'*20)\n",
        "        \n",
        "        for phase in ['train', 'val']:           \n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            epoch_loss = 0.0\n",
        "            epoch_corrects = 0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloader_dict[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict()\n",
        "                \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    return model"
      ],
      "metadata": {
        "id": "v4biO-jewPXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_epoch = 10\n",
        "model = train_model(model, dataloader_dict, criterion, optimizer, num_epoch)"
      ],
      "metadata": {
        "id": "2mSSF4T6w4dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "id_list = []\n",
        "pred_list = []\n",
        "_id=0\n",
        "with torch.no_grad():\n",
        "    for test_path in tqdm(test_images_filepaths):\n",
        "        img = Image.open(test_path)\n",
        "        _id =test_path.split('/')[-1].split('.')[1]\n",
        "        transform = ImageTransform(size, mean, std)\n",
        "        img = transform(img, phase='val')\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        outputs = model(img)\n",
        "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()        \n",
        "        id_list.append(_id)\n",
        "        pred_list.append(preds[0])\n",
        "       \n",
        "res = pd.DataFrame({\n",
        "    'id': id_list,\n",
        "    'label': pred_list\n",
        "})\n",
        "\n",
        "res.sort_values(by='id', inplace=True)\n",
        "res.reset_index(drop=True, inplace=True)\n",
        "\n",
        "res.to_csv('LesNet.csv', index=False)"
      ],
      "metadata": {
        "id": "3cWzlyeaxT32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.head(10)"
      ],
      "metadata": {
        "id": "93qnEiRH45yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ = classes = {0:'cat', 1:'dog'}\n",
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
        "    rows = len(images_filepaths) // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths):\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        a = random.choice(res['id'].values)    \n",
        "        label = res.loc[res['id'] == a, 'label'].values[0]\n",
        "        if label > 0.5:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(class_[label])\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0fwBreoT5BB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepaths) "
      ],
      "metadata": {
        "id": "h5GN1kOC5Hhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AlexNet"
      ],
      "metadata": {
        "id": "BukUIRhj5c1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9YqBfdRi5sT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform():    \n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.data_transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(resize),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        }\n",
        "        \n",
        "    def __call__(self, img, phase):\n",
        "        return self.data_transform[phase](img)"
      ],
      "metadata": {
        "id": "tzRcEcHM53AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_directory = '/content/drive/MyDrive/Pytorch/chap06/data/Cat'\n",
        "dog_directory = '/content/drive/MyDrive/Pytorch/chap06/data/Dog'\n",
        "\n",
        "cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])   \n",
        "dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
        "images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]    \n",
        "correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]    \n",
        "\n",
        "random.seed(42)    \n",
        "random.shuffle(correct_images_filepaths)\n",
        "#train_images_filepaths = correct_images_filepaths[:20000] #성능을 향상시키고 싶다면 훈련 데이터셋을 늘려서 테스트해보세요   \n",
        "#val_images_filepaths = correct_images_filepaths[20000:-10] #훈련과 함께 검증도 늘려줘야 합니다\n",
        "train_images_filepaths = correct_images_filepaths[:400]    \n",
        "val_images_filepaths = correct_images_filepaths[400:-10] \n",
        "test_images_filepaths = correct_images_filepaths[-10:]    \n",
        "print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))"
      ],
      "metadata": {
        "id": "aSwLc--C6DvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogvsCatDataset(Dataset):    \n",
        "    def __init__(self, file_list, transform=None, phase='train'):    \n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):        \n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img, self.phase)\n",
        "        \n",
        "        label = img_path.split('/')[-1].split('.')[0]\n",
        "        if label == 'dog':\n",
        "            label = 1\n",
        "        elif label == 'cat':\n",
        "            label = 0\n",
        "\n",
        "        return img_transformed, label"
      ],
      "metadata": {
        "id": "OPeCuMSo6Woz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 256\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "tPm90nKu6p4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')\n",
        "val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')\n",
        "test_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')\n",
        "\n",
        "index = 0\n",
        "print(train_dataset.__getitem__(index)[0].size())\n",
        "print(train_dataset.__getitem__(index)[1])"
      ],
      "metadata": {
        "id": "SATiedpY6uQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
        "\n",
        "batch_iterator = iter(train_dataloader)\n",
        "inputs, label = next(batch_iterator)\n",
        "print(inputs.size())\n",
        "print(label)"
      ],
      "metadata": {
        "id": "88vgpFly7OXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eSCTyQVh7TvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "g1Y0PzM-7dRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "PC8eBo3K7uAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 256, 256))"
      ],
      "metadata": {
        "id": "39yy-P-O7ysl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):    \n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epoch))\n",
        "        print('-'*20)\n",
        "        \n",
        "        for phase in ['train', 'val']:            \n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            epoch_loss = 0.0\n",
        "            epoch_corrects = 0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloader_dict[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "   \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    return model"
      ],
      "metadata": {
        "id": "aAhy7B_78Adb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "model = train_model(model, dataloader_dict, criterion, optimizer, num_epoch)"
      ],
      "metadata": {
        "id": "HVuEZoj08IR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "id_list = []\n",
        "pred_list = []\n",
        "_id=0\n",
        "with torch.no_grad():\n",
        "    for test_path in tqdm(test_images_filepaths):\n",
        "        img = Image.open(test_path)\n",
        "        _id =test_path.split('/')[-1].split('.')[1]\n",
        "        transform = ImageTransform(size, mean, std)\n",
        "        img = transform(img, phase='val')\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        outputs = model(img)\n",
        "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n",
        "        \n",
        "        id_list.append(_id)\n",
        "        pred_list.append(preds[0])\n",
        "       \n",
        "res = pd.DataFrame({\n",
        "    'id': id_list,\n",
        "    'label': pred_list\n",
        "})\n",
        "res.to_csv('alexnet.csv', index=False)"
      ],
      "metadata": {
        "id": "ACj16k_j8Rus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.head(10)"
      ],
      "metadata": {
        "id": "MkUG17T78XlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ = classes = {0:'cat', 1:'dog'}\n",
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
        "    rows = len(images_filepaths) // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths):\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        a = random.choice(res['id'].values)    \n",
        "        label = res.loc[res['id'] == a, 'label'].values[0]\n",
        "        if label > 0.5:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(class_[label])\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iMzBT1sX8dYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepaths) "
      ],
      "metadata": {
        "id": "jQ9L-jeq8gP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGGNET"
      ],
      "metadata": {
        "id": "7g1PBJgA8pKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as Datasets\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "eWkRCfTw8syH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, output_dim):\n",
        "        super().__init__()        \n",
        "        self.features = features        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(7)        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(h)\n",
        "        return x, h"
      ],
      "metadata": {
        "id": "D7mdlZQ7_3_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, \n",
        "                512, 'M']\n",
        "\n",
        "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "                512, 512, 512, 512, 'M']"
      ],
      "metadata": {
        "id": "65I11sxoAdZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vgg_layers(config, batch_norm):    \n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    \n",
        "    for c in config:\n",
        "        assert c == 'M' or isinstance(c, int)\n",
        "        if c == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size = 2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace = True)]\n",
        "            in_channels = c\n",
        "            \n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "Ll8IKpbjAfnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)"
      ],
      "metadata": {
        "id": "FMC2FoDbBEfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vgg11_layers)"
      ],
      "metadata": {
        "id": "bz7CfrrJBGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIM = 2\n",
        "model = VGG(vgg11_layers, OUTPUT_DIM).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "8dQKrxraBKFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "pretrained_model = models.vgg11_bn(pretrained = True)\n",
        "print(pretrained_model)"
      ],
      "metadata": {
        "id": "LWim5GkaBotI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                           transforms.Resize((256, 256)),\n",
        "                           transforms.RandomRotation(5),\n",
        "                           transforms.RandomHorizontalFlip(0.5),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.Resize((256, 256)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "cjZqw4EHB0yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip /content/drive/MyDrive/Pytorch/chap06/catanddog.zip -d catanddog/  #catanddog 폴더 만들어 압축 풀기"
      ],
      "metadata": {
        "id": "KF1yVSPSCORF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Pytorch/chap06/data/catanddog/train'\n",
        "test_path = '/content/drive/MyDrive/Pytorch/chap06/data/catanddog/test'\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    train_path,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "    test_path,\n",
        "    transform=test_transforms\n",
        ")\n",
        "\n",
        "print(len(train_dataset)), print(len(test_dataset))"
      ],
      "metadata": {
        "id": "goce9CBuCm2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALID_RATIO = 0.9\n",
        "n_train_examples = int(len(train_dataset) * VALID_RATIO)\n",
        "n_valid_examples = len(train_dataset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(train_dataset, \n",
        "                                           [n_train_examples, n_valid_examples])"
      ],
      "metadata": {
        "id": "7qe0RymuDAZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ],
      "metadata": {
        "id": "uceTe4jcDBVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "lX2GGJhVDDf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32 #메모리 부족으로 오류가 발생하여 배치 사이즈를 줄였습니다\n",
        "train_iterator = data.DataLoader(train_data, \n",
        "                                 shuffle = True, \n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "valid_iterator = data.DataLoader(valid_data, \n",
        "                                 abatch_size = BATCH_SIZE)\n",
        "\n",
        "test_iterator = data.DataLoader(test_dataset, \n",
        "                                batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "OBjRDdHnDKZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 1e-7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "AlbaMoqfDleY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "metadata": {
        "id": "FVvdRhIHDslt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()    \n",
        "    for (x, y) in iterator:        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()                \n",
        "        y_pred, _ = model(x)        \n",
        "        loss = criterion(y_pred, y)       \n",
        "        acc = calculate_accuracy(y_pred, y)        \n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "AfWA5yiUDu6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device):    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()    \n",
        "    with torch.no_grad():        \n",
        "        for (x, y) in iterator:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_pred, _ = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "QV4zGLF7D22s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "vX-RRQ0VD7fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(EPOCHS):    \n",
        "    start_time = time.monotonic()    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'VGG-model.pt')\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Valid. Loss: {valid_loss:.3f} |  Valid. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "2ZEaeQu5EDvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('VGG-model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "fP3T4BDbEKkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, iterator):\n",
        "    model.eval()\n",
        "    images = []\n",
        "    labels = []\n",
        "    probs = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (x, y) in iterator:\n",
        "            x = x.to(device)\n",
        "            y_pred, _ = model(x)\n",
        "            y_prob = F.softmax(y_pred, dim = -1)\n",
        "            top_pred = y_prob.argmax(1, keepdim = True)\n",
        "            images.append(x.cpu())\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(y_prob.cpu())\n",
        "\n",
        "    images = torch.cat(images, dim = 0)\n",
        "    labels = torch.cat(labels, dim = 0)\n",
        "    probs = torch.cat(probs, dim = 0)\n",
        "    return images, labels, probs"
      ],
      "metadata": {
        "id": "TSA7vLQHEPfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels, probs = get_predictions(model, test_iterator)\n",
        "pred_labels = torch.argmax(probs, 1)\n",
        "corrects = torch.eq(labels, pred_labels)\n",
        "correct_examples = []\n",
        "\n",
        "for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
        "    if correct:\n",
        "        correct_examples.append((image, label, prob))\n",
        "\n",
        "correct_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"
      ],
      "metadata": {
        "id": "Rl3ZmmcJEiNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(image):\n",
        "    image_min = image.min()\n",
        "    image_max = image.max()\n",
        "    image.clamp_(min = image_min, max = image_max)\n",
        "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
        "    return image"
      ],
      "metadata": {
        "id": "tlURiwh9E2vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_most_correct(correct, classes, n_images, normalize = True):\n",
        "    rows = int(np.sqrt(n_images))\n",
        "    cols = int(np.sqrt(n_images))\n",
        "    fig = plt.figure(figsize = (25, 20))\n",
        "    for i in range(rows*cols):\n",
        "        ax = fig.add_subplot(rows, cols, i+1)        \n",
        "        image, true_label, probs = correct[i]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        true_prob = probs[true_label]\n",
        "        correct_prob, correct_label = torch.max(probs, dim = 0)\n",
        "        true_class = classes[true_label]\n",
        "        correct_class = classes[correct_label]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax.imshow(image.cpu().numpy())\n",
        "        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\n",
        "                     f'pred label: {correct_class} ({correct_prob:.3f})')\n",
        "        ax.axis('off')\n",
        "        \n",
        "    fig.subplots_adjust(hspace = 0.4)"
      ],
      "metadata": {
        "id": "TdYFiaLGFAAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = test_dataset.classes\n",
        "N_IMAGES = 5\n",
        "plot_most_correct(correct_examples, classes, N_IMAGES)"
      ],
      "metadata": {
        "id": "zclwaBtPFGsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet"
      ],
      "metadata": {
        "id": "XGVDWlWvF50_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "네트워크 깊이가 깊다고 해서 무조건 성능이 좋아지지 않는다. 이러한 문제를 해결하기 위함."
      ],
      "metadata": {
        "id": "zl1-gOeeGDIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "qD6zW8P5FGil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
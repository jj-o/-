{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1GSj2nOYAhQH6dZl0HKLFkl4oBTG0Tf4v",
      "authorship_tag": "ABX9TyPqbvt0mF/KBSKIqrBCOoBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jj-o/Pytorch/blob/main/ConvNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WGRyeSalZpK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NN6XDntLniXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset  = torchvision.datasets.FashionMNIST(\"FashionMNIST/\", download=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_dataset  = torchvision.datasets.FashionMNIST(\"FashionMNIST/\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()])) "
      ],
      "metadata": {
        "id": "cSd_EzpEnjF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=100)"
      ],
      "metadata": {
        "id": "x7xpFKgunxZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
        "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}\n",
        "\n",
        "fig = plt.figure(figsize=(8,8));\n",
        "columns = 4;\n",
        "rows = 5;\n",
        "for i in range(1, columns*rows +1):\n",
        "    img_xy = np.random.randint(len(train_dataset));\n",
        "    img = train_dataset[img_xy][0][0,:,:]\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.title(labels_map[train_dataset[img_xy][1]])\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_CYQwdwAn1Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionDNN,self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=784,out_features=256)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=256,out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128,out_features=10)\n",
        "\n",
        "    def forward(self,input_data):\n",
        "        out = input_data.view(-1, 784)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.drop(out)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "XWgJKYpUn8hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001;\n",
        "model = FashionDNN();\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
        "print(model)"
      ],
      "metadata": {
        "id": "m1PQlHTGoDEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "        train = Variable(images.view(100, 1, 28, 28))\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        outputs = model(train)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "    \n",
        "        if not (count % 50):    \n",
        "            total = 0\n",
        "            correct = 0        \n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels_list.append(labels)            \n",
        "                test = Variable(images.view(100, 1, 28, 28))            \n",
        "                outputs = model(test)            \n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                predictions_list.append(predictions)\n",
        "                correct += (predictions == labels).sum()            \n",
        "                total += len(labels)\n",
        "            \n",
        "            accuracy = correct * 100 / total\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        \n",
        "        if not (count % 500):\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "id": "iTfD2-hnoixz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):    \n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )       \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )        \n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)       \n",
        "        return out"
      ],
      "metadata": {
        "id": "IF8V8mvPpJwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001;\n",
        "model = FashionCNN();\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
        "print(model)"
      ],
      "metadata": {
        "id": "gS33Yv3JpiZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "        train = Variable(images.view(100, 1, 28, 28))\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        outputs = model(train)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "    \n",
        "        if not (count % 50):    \n",
        "            total = 0\n",
        "            correct = 0        \n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels_list.append(labels)            \n",
        "                test = Variable(images.view(100, 1, 28, 28))            \n",
        "                outputs = model(test)            \n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                predictions_list.append(predictions)\n",
        "                correct += (predictions == labels).sum()            \n",
        "                total += len(labels)\n",
        "            \n",
        "            accuracy = correct * 100 / total\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        \n",
        "        if not (count % 500):\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "id": "VBlb4noepkvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전이학습\n",
        "전이학습 : ImageNet처럼 아주 큰 데이터셋을 써서 훈련된 모델의 가중치를 가져와 우리가 해결하려는 과제에 맞게 보정해서 사용하는 것"
      ],
      "metadata": {
        "id": "ZXCj6ICvpxdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "id": "cYJhCX0EqSIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import glob\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IGswdPO_qWoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Pytorch/chap05/data/catanddog.zip -d catanddog/"
      ],
      "metadata": {
        "id": "I5Sk8yRBqm5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'catanddog/train/'\n",
        "transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize([256, 256]),\n",
        "                    transforms.RandomResizedCrop(224),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                ])\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    data_path,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(len(train_dataset))"
      ],
      "metadata": {
        "id": "JWakQofRqynp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "samples, labels = iter(train_loader).next()\n",
        "classes = {0:'cat', 1:'dog'}\n",
        "fig = plt.figure(figsize=(16,24))\n",
        "for i in range(24):\n",
        "    a = fig.add_subplot(4,6,i+1)\n",
        "    a.set_title(classes[labels[i].item()])\n",
        "    a.axis('off')\n",
        "    a.imshow(np.transpose(samples[i].numpy(), (1,2,0)))\n",
        "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
      ],
      "metadata": {
        "id": "U4jDXn_ut4H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "id": "3Bo80czSxFa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting=True):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "set_parameter_requires_grad(resnet18)"
      ],
      "metadata": {
        "id": "fDIlKVUWxFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.fc = nn.Linear(512, 2)"
      ],
      "metadata": {
        "id": "7HJ-o38oxPgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in resnet18.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "id": "YfGerNeHxTE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = torch.nn.Linear(512, 2)\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(model.fc.parameters())\n",
        "cost = torch.nn.CrossEntropyLoss()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "AcA4SiQsxVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=13, is_train=True):\n",
        "    since = time.time()    \n",
        "    acc_history = []\n",
        "    loss_history = []\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            model.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
        "\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "\n",
        "        acc_history.append(epoch_acc.item())\n",
        "        loss_history.append(epoch_loss)        \n",
        "        torch.save(model.state_dict(), os.path.join('catanddog/', '{0:0=2d}.pth'.format(epoch)))\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Acc: {:4f}'.format(best_acc))    \n",
        "    return acc_history, loss_history"
      ],
      "metadata": {
        "id": "Ms3rEKGlxe32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name,param in resnet18.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)\n",
        "            \n",
        "optimizer = optim.Adam(params_to_update)"
      ],
      "metadata": {
        "id": "L8I0EOrYxlw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_acc_hist, train_loss_hist = train_model(resnet18, train_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "3KoOklQFxqAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = '/content/drive/MyDrive/Pytorch/chap05/data/catanddog/test'\n",
        "\n",
        "transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize(224),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                ])\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=test_path,\n",
        "    transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "qgt-ltFbxupa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, dataloaders, device):\n",
        "    since = time.time()    \n",
        "    acc_history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    saved_models = glob.glob('catanddog/' + '*.pth')\n",
        "    saved_models.sort()\n",
        "    print('saved_model', saved_models)\n",
        "\n",
        "    for model_path in saved_models:\n",
        "        print('Loading model', model_path)\n",
        "\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            _, preds = torch.max(outputs.data, 1)           \n",
        "            preds[preds >= 0.5] = 1\n",
        "            preds[preds < 0.5] = 0\n",
        "            running_corrects += preds.eq(labels).int().sum()\n",
        "            \n",
        "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
        "        print('Acc: {:.4f}'.format(epoch_acc))\n",
        "        \n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "\n",
        "        acc_history.append(epoch_acc.item())\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Acc: {:4f}'.format(best_acc))\n",
        "    \n",
        "    return acc_history"
      ],
      "metadata": {
        "id": "9D44LLS9x4gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc_hist = eval_model(resnet18, test_loader, device)"
      ],
      "metadata": {
        "id": "Y1rk7j8oyFI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_acc_hist)\n",
        "plt.plot(val_acc_hist)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GQXNzsULyK86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_hist)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6dzZYyoCyMXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def im_convert(tensor):  \n",
        "    image=tensor.clone().detach().numpy()  \n",
        "    image=image.transpose(1,2,0)  \n",
        "    image=image*(np.array((0.5,0.5,0.5))+np.array((0.5,0.5,0.5)))  \n",
        "    image=image.clip(0,1)  \n",
        "    return image  "
      ],
      "metadata": {
        "id": "Z3oncKvxyYH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {0:'cat', 1:'dog'}\n",
        "\n",
        "dataiter=iter(test_loader)  \n",
        "images,labels=dataiter.next()  \n",
        "output=model(images)  \n",
        "_,preds=torch.max(output,1) \n",
        "\n",
        "fig=plt.figure(figsize=(25,4))  \n",
        "for idx in np.arange(20):  \n",
        "    ax=fig.add_subplot(2,10,idx+1,xticks=[],yticks=[])  \n",
        "    plt.imshow(im_convert(images[idx]))  \n",
        "    a.set_title(classes[labels[i].item()])\n",
        "    ax.set_title(\"{}({})\".format(str(classes[preds[idx].item()]),str(classes[labels[idx].item()])),color=(\"green\" if preds[idx]==labels[idx] else \"red\"))  \n",
        "plt.show()  \n",
        "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
      ],
      "metadata": {
        "id": "cPPbkuq_yn9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}